{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following if first time running nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\emcco\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\emcco\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "class_names = []\n",
    "classes = []\n",
    "documents = []\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    full_text = \"\"\n",
    "    for pattern in intent['patterns']:\n",
    "        full_text = full_text + \" \" + pattern\n",
    "    full_text = re.sub(r'[.?!,:;()\\-\\n\\d]',' ', full_text.lower())\n",
    "    # adding documents\n",
    "    documents.append((full_text, intent['tag']))\n",
    "\n",
    "    # adding classes to our class list\n",
    "    if intent['tag'] not in class_names:\n",
    "        class_names.append(intent['tag'])\n",
    "        classes.append((intent['tag'], intent['context']))\n",
    "\n",
    "pickle.dump(documents,open('documents.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(' hi there how are you hey good day hi hello yo', 'greeting'), (' bye see you later goodbye till next time quit peace out', 'goodbye'), (\" thanks thank you that's helpful awesome  thanks thanks for helping me appreciate it good call thank you\", 'thanks'), (' how you could help me  what can you do  what help you provide  how you can be helpful  what support is offered what options do you have  help help me instructions huh', 'options'), (' connect to spotify login sign in to spotify spotify open spotify get spotify data pull data', 'connect_spotify'), (' how you could help me  what can you do  what help you provide  how you can be helpful  what support is offered what options do you have  help help me instructions huh', 'options_2'), (' what artists do i like  my top artists what are my top artists what artists do i listen to the most who are my favorite artists who are my favorite musicians what musicians do i like what acts do i enjoy listening to ', 'top_artists'), (' what tracks do i like  what songs do i like  my top tracks what are my top tracks my top songs what are my top songs what songs do i listen to the most what are my favorite tracks what are my favorite songs what tracks do i enjoy listening to  what songs do i enjoy listening to ', 'top_tracks'), (' what music do i like  what music do i enjoy listening to  what do i enjoy listening to  my top music what do i listen to what do i like what do i listen to the most what is my favorite music what are my music tastes what are my listening preferences what are my musical preferences', 'listening_preferences'), (' what music should i listen to  what music would you recommend  what songs do you recommend  what do you recommend  what would i like can you recommend a song to me can you recommend me music what else would i like i dont know what to listen to i want to find new music', 'recommend_music')]\n10\n[('greeting', 'general'), ('goodbye', 'general'), ('thanks', 'general'), ('options', 'main'), ('connect_spotify', 'general'), ('options_2', 'hydrated'), ('top_artists', 'hydrated'), ('top_tracks', 'hydrated'), ('listening_preferences', 'hydrated'), ('recommend_music', 'hydrated')]\n10\n"
     ]
    }
   ],
   "source": [
    "print(documents[:10])\n",
    "print(len(documents))\n",
    "\n",
    "print(classes[:10])\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "docs shape: (11, 84)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "documents_raw = [d[0] for d in documents]\n",
    "\n",
    "query = \"what can you do\"\n",
    "documents_raw.append(query)\n",
    "\n",
    "tfidf_docs = tfidf_vectorizer.fit_transform(documents_raw)\n",
    "print('docs shape:', tfidf_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06942139602513236\n0.07190573158174676\n0.09652389999247564\n0.5536496060195553\n0.0\n0.5536496060195553\n0.2512911794704487\n0.28464084359130293\n0.37484630309320416\n0.42479359410902484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "arr = list(cosine_similarity(tfidf_docs[-1], tfidf_docs)[0][:-1])\n",
    "for p in arr:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "arr.index(max(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "34 documents\n5 classes [('greeting', 'general'), ('goodbye', 'general'), ('thanks', 'general'), ('options', 'general'), ('connect_spotify', 'general')]\n54 unique lemmatized words [\"'s\", 'appreciate', 'are', 'awesome', 'be', 'bye', 'call', 'can', 'connect', 'could', 'data', 'day', 'do', 'for', 'get', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'how', 'in', 'is', 'it', 'later', 'login', 'me', 'next', 'offered', 'open', 'option', 'out', 'peace', 'provide', 'pull', 'quit', 'see', 'sign', 'spotify', 'support', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'what', 'yo', 'you']\n"
    }
   ],
   "source": [
    "# Parse documents\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if len(w) > 1]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1648db51aef089344c971c491da9bd2a76611211bee3d4e18e6b5ac523d0137a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}